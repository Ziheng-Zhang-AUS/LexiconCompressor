# lexicon_compressor_model.py
import torch
import torch.nn as nn
from typing import List, Optional, Tuple, Dict, Any
from transformers.models.qwen3.modeling_qwen3 import Qwen3Config
from row_column_attention_stack import RowColumnAttentionStack

class LexiconCompressorModel(nn.Module):
    def __init__(self, config: Qwen3Config, num_attention_layers: int = 2):
        """
        è¯æ±‡è¡¨å‹ç¼©å™¨æ¨¡å‹
        
        Args:
            config: Qwen3é…ç½®
            num_attention_layers: è¡Œåˆ—æ³¨æ„åŠ›å±‚æ•°é‡
        """
        super().__init__()
        self.config = config
        self.num_attention_layers = num_attention_layers
        
        # è¡Œåˆ—æ³¨æ„åŠ›å †æ ˆ
        self.attention_stack = RowColumnAttentionStack(config, num_attention_layers)
        
        # embeddingå±‚å‚æ•°å ä½ç¬¦ï¼ˆå®é™…æƒé‡é€šè¿‡load_embeddings_weightsåŠ è½½ï¼‰
        self.embeddings = None
        self._embeddings_loaded = False
        
    def load_embeddings_weights(self, embedding_weights: Dict[str, torch.Tensor]):
        """
        åŠ è½½embeddingå±‚æƒé‡
        
        Args:
            embedding_weights: embeddingå±‚çš„æƒé‡å­—å…¸
        """
        # åˆ›å»ºembeddingå±‚å¹¶åŠ è½½æƒé‡
        vocab_size = embedding_weights['weight'].shape[0]
        hidden_size = embedding_weights['weight'].shape[1]
        
        self.embeddings = nn.Embedding(vocab_size, hidden_size)
        self.embeddings.load_state_dict(embedding_weights)
        self.embeddings.to(torch.device("cuda")) 
        self._embeddings_loaded = True
    
    def load_attention_weights(self, attention_weights_list: List[Tuple[Dict, Dict]]):
        """
        åŠ è½½æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„æƒé‡
        
        Args:
            attention_weights_list: æ³¨æ„åŠ›å±‚æƒé‡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯(row_weights, col_weights)å…ƒç»„
        """
        self.attention_stack.load_weights_once(attention_weights_list)
    
    def tokenize_and_embed(self, token_ids_list: List[List[int]]) -> List[torch.Tensor]:
        """
        å°†token IDsè½¬æ¢ä¸ºåµŒå…¥å‘é‡
        
        Args:
            token_ids_list: List[List[int]] æ¯è¡Œçš„token IDs
            
        Returns:
            List[torch.Tensor]: æ¯è¡Œçš„åµŒå…¥å‘é‡åˆ—è¡¨
        """
        if not self._embeddings_loaded:
            raise ValueError("Embeddings weights must be loaded first")
        
        embedded_rows = []
        for token_ids in token_ids_list:
            # è½¬æ¢ä¸ºtensor
            token_tensor = torch.tensor(token_ids, dtype=torch.long, device=self.embeddings.weight.device)
            # è·å–åµŒå…¥
            embedded = self.embeddings(token_tensor)  # shape: [seq_len, hidden_size]
            embedded_rows.append(embedded)
        
        return embedded_rows
    
    def forward(
        self,
        token_ids_list: List[List[int]],  # è¾“å…¥çš„token IDsåˆ—è¡¨
        attention_weights: Optional[List[Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]]] = None,
        embeddings_weights: Optional[Dict[str, torch.Tensor]] = None,
        row_attention_masks: Optional[List[List[torch.Tensor]]] = None,
        column_attention_masks: Optional[List[torch.Tensor]] = None,
        row_position_embeddings: Optional[List[List[Tuple[torch.Tensor, torch.Tensor]]]] = None,
        column_position_embeddings: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
    ):
        """
        å‰å‘ä¼ æ’­ - è¯æ±‡è¡¨å‹ç¼©
        
        Args:
            token_ids_list: List[List[int]] æ¯è¡Œçš„token IDs
            attention_weights: æ³¨æ„åŠ›å±‚æƒé‡åˆ—è¡¨
            embeddings_weights: embeddingå±‚æƒé‡
            å…¶ä»–æ³¨æ„åŠ›ç›¸å…³å‚æ•°...
        """
        
        # åŠ è½½æƒé‡ï¼ˆå¦‚æœæä¾›äº†çš„è¯ï¼‰
        if embeddings_weights is not None and not self._embeddings_loaded:
            self.load_embeddings_weights(embeddings_weights)
        
        if attention_weights is not None:
            self.load_attention_weights(attention_weights)
        
        # 1. Tokenize and embed
        embedded_rows = self.tokenize_and_embed(token_ids_list)
        
        # 2. é€šè¿‡è¡Œåˆ—æ³¨æ„åŠ›å †æ ˆ
        compressed_rows = self.attention_stack(
            embedded_rows=embedded_rows,
            row_attention_masks=row_attention_masks,
            column_attention_masks=column_attention_masks,
            row_position_embeddings=row_position_embeddings,
            column_position_embeddings=column_position_embeddings
        )
        
        return compressed_rows



def main():
    import torch
    from transformers import AutoTokenizer, Qwen3ForCausalLM
    from tokenization_lexicon import LexiconTokenizer

    MODEL_NAME = "Qwen/Qwen3-0.6B"
    CSV_PATH   = "cleaned_lexicon_tiny.csv"
    COLUMNS    = ["lexical_unit", "pos", "gloss", "variant"]
    NUM_LAYERS = 6
    SHOW_N     = 100

    torch.set_grad_enabled(False)

    # 1) tokenizer + [COMP]
    tok = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    tok.add_tokens(["[COMP]"], special_tokens=False)
    comp_id = tok.convert_tokens_to_ids("[COMP]")

    # 2) CSV â†’ token idsï¼ˆæ¯è¡Œå‰ç½® [COMP]ï¼‰
    lt = LexiconTokenizer(
        csv_file_path=CSV_PATH,
        tokenizer=tok,
        column_names=COLUMNS,
        compress_token_id=comp_id,
        delimiter=",",
        add_special_tokens=False
    )
    token_ids_list = lt.process_lexicon()[:SHOW_N]
    assert token_ids_list, "No entries parsed from CSV."

    # 3) Qwen æƒé‡ï¼ˆembedding / å±‚ / RoPEï¼‰
    qwen   = Qwen3ForCausalLM.from_pretrained(MODEL_NAME)
    cfg    = qwen.config
    rotary = qwen.model.rotary_emb
    H      = cfg.hidden_size

    lcm = LexiconCompressorModel(config=cfg, num_attention_layers=NUM_LAYERS)
    emb_weights = qwen.get_input_embeddings().state_dict()
    attn_weights = []
    for i in range(NUM_LAYERS):
        row_sd = qwen.model.layers[2*i].state_dict()
        col_sd = qwen.model.layers[2*i+1].state_dict()
        attn_weights.append((row_sd, col_sd))

    # --------- helpers: æ„é€  RoPE / maskï¼Œå¹¶å‰å‘ä¸€æ¬¡ ----------
    def full_vis_mask(L: int):
        return torch.zeros(1, 1, L, L, dtype=torch.float32)

    def row_rope(L: int):
        dummy = torch.zeros(1, L, H)
        pos   = torch.arange(L).unsqueeze(0)
        return rotary(dummy, pos)  # æœ‰åº RoPE

    def col_identity_rope(N: int):
        dummy = torch.zeros(1, N, H)
        pos0  = torch.zeros(1, N, dtype=torch.long)
        return rotary(dummy, pos0)  # identityï¼ˆæ— åºï¼‰

    def build_inputs(tids):
        # è¡Œï¼šæ¯å±‚éƒ½è¦ä¸€ä»½ï¼ˆç®€å•å¤åˆ¶ï¼‰
        row_pos = []; row_msk = []
        for _ in range(NUM_LAYERS):
            row_pos.append([row_rope(len(x)) for x in tids])
            row_msk.append([full_vis_mask(len(x)) for x in tids])
        # åˆ—ï¼šæ¯å±‚ä¸€ä»½
        N = len(tids)
        col_pos = [col_identity_rope(N) for _ in range(NUM_LAYERS)]
        col_msk = [full_vis_mask(N)     for _ in range(NUM_LAYERS)]
        return row_pos, row_msk, col_pos, col_msk

    def fwd_once(tids):
        row_pos, row_msk, col_pos, col_msk = build_inputs(tids)
        out_rows = lcm(
            token_ids_list=tids,
            attention_weights=attn_weights,
            embeddings_weights=emb_weights,
            row_attention_masks=row_msk,
            column_attention_masks=col_msk,
            row_position_embeddings=row_pos,
            column_position_embeddings=col_pos
        )
        # å–æ¯è¡Œå®¹å™¨ï¼ˆè¡Œé¦– [COMP]ï¼‰å‘é‡
        heads = torch.stack([r[0].detach().cpu() for r in out_rows], dim=0)
        return out_rows, heads
    # ------------------------------------------------------------

    print("Forward once (baseline)...")
    out_rows, heads = fwd_once(token_ids_list)
    for i, (ids, row) in enumerate(zip(token_ids_list, out_rows)):
        print(f"Row {i}: ids_len={len(ids)} -> out_shape={tuple(row.shape)}; head_norm={row[0].norm().item():.4f}")

    # ============== æµ‹è¯• 1ï¼šåˆ—æ— åºï¼ˆåˆ—åº”å¯¹è¡Œé¡ºåºä¸æ•æ„Ÿï¼‰ ==============
    import random
    perm = list(range(len(token_ids_list)))
    random.shuffle(perm)
    perm_tids = [token_ids_list[i] for i in perm]
    _, heads_perm = fwd_once(perm_tids)

    assert torch.allclose(heads_perm, heads[perm], atol=1e-5, rtol=1e-5), \
        "Column invariance FAILED: shuffling rows changed container vectors beyond permutation."
    print("Test#1 Column invariance: OK âœ…")

    # ============ æµ‹è¯• 2ï¼šè¡Œæœ‰åºï¼ˆè¡Œå†…äº¤æ¢ token åº”æ”¹å˜è¾“å‡ºï¼‰ ============
    # é€‰ç¬¬ä¸€è¡Œï¼Œå°è¯•äº¤æ¢ä¸¤ä¸ªé [COMP] çš„ä½ç½®ï¼ˆç¡®ä¿é•¿åº¦â‰¥3ï¼‰
    swapped_tids = [x[:] for x in token_ids_list]
    if len(swapped_tids[0]) >= 3:
        i0, j0 = 1, 2  # äº¤æ¢ç¬¬1/2ä¸ªçœŸå® tokenï¼ˆ0 æ˜¯ [COMP]ï¼‰
        swapped_tids[0][i0], swapped_tids[0][j0] = swapped_tids[0][j0], swapped_tids[0][i0]
        _, heads_swapped = fwd_once(swapped_tids)
        assert not torch.allclose(heads_swapped[0], heads[0], atol=1e-6), \
            "Row order FAILED: swapping tokens did not change row container."
        print("Test#2 Row order sensitivity: OK âœ…")
    else:
        print("Test#2 Row order sensitivity: SKIP (row too short)")

    # ====== æµ‹è¯• 3ï¼šéå› æœï¼ˆå³ä¾§ä¿¡æ¯èƒ½å½±å“å·¦ä¾§å®¹å™¨ [COMP]ï¼‰ ======
    # åœ¨ç¬¬ä¸€è¡Œï¼ŒæŠŠé å³çš„ä¸€ä¸ª token æ›¿æ¢æˆéšæœºå‘é‡å¯¹åº”çš„â€œå½±å­ token idâ€ï¼ˆè¿™é‡Œç®€å•ç”¨é‡å¤äº¤æ¢æ¥æ¨¡æ‹Ÿå˜åŒ–ï¼‰
    # æ›´ç›´æ¥çš„åšæ³•æ˜¯æ›¿æ¢ä¸€ä¸ªå³ä¾§ tokenï¼ˆi>=2ï¼‰ä¸ºå¦ä¸€ä¸ª id
    noncausal_tids = [x[:] for x in token_ids_list]
    if len(noncausal_tids[0]) >= 4:
        # äº¤æ¢æ›´é å³çš„ä½ç½®ï¼ŒæŸ¥çœ‹ [COMP] æ˜¯å¦å—å½±å“
        i1, j1 = 2, 3
        noncausal_tids[0][i1], noncausal_tids[0][j1] = noncausal_tids[0][j1], noncausal_tids[0][i1]
        _, heads_noncausal = fwd_once(noncausal_tids)
        assert not torch.allclose(heads_noncausal[0], heads[0], atol=1e-6), \
            "Non-causal FAILED: changing right-side tokens did not affect [COMP]."
        print("Test#3 Non-causal (full visibility): OK âœ…")
    else:
        print("Test#3 Non-causal (full visibility): SKIP (row too short)")

    print("All tests passed ğŸ‰")

if __name__ == "__main__":
    main()